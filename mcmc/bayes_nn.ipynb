{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6095a29152a3dc59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:02:51.015881Z",
     "start_time": "2024-05-11T18:02:51.000327Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "from mcmc import run_lmc_numpyro\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")  # noqa: E402\n",
    "\n",
    "\n",
    "# a two-layer bayesian neural network with computational flow\n",
    "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
    "# hidden units. (note we indicate tensor dimensions in the comments)\n",
    "def model(X, Y, D_H, D_Y=1):\n",
    "    N, D_X = X.shape\n",
    "\n",
    "    # sample first layer (we put unit normal priors on all weights)\n",
    "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
    "    assert w1.shape == (D_X, D_H), f\"Expected shape {(D_X, D_H)}, got {w1.shape}\"\n",
    "    z1 = jnp.tanh(jnp.matmul(X, w1))  # <= first layer of activations\n",
    "    assert z1.shape == (N, D_H)\n",
    "\n",
    "    # sample second layer\n",
    "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
    "    assert w2.shape == (D_H, D_H)\n",
    "    z2 = jnp.tanh(jnp.matmul(z1, w2))  # <= second layer of activations\n",
    "    assert z2.shape == (N, D_H)\n",
    "\n",
    "    # sample final layer of weights and neural network output\n",
    "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
    "    assert w3.shape == (D_H, D_Y)\n",
    "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
    "    assert z3.shape == (N, D_Y)\n",
    "\n",
    "    if Y is not None:\n",
    "        assert z3.shape == Y.shape\n",
    "\n",
    "    # we put a prior on the observation noise\n",
    "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
    "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
    "\n",
    "    # observe data\n",
    "    with numpyro.plate(\"data\", N):\n",
    "        # note we use to_event(1) because each observation has shape (1,)\n",
    "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
    "\n",
    "\n",
    "# helper function for HMC inference\n",
    "def run_inference(model, args, rng_key, X, Y, D_H):\n",
    "    start = time.time()\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=args.num_warmup,\n",
    "        num_samples=args.num_samples,\n",
    "        num_chains=args.num_chains,\n",
    "        chain_method=\"vectorized\",\n",
    "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    "    )\n",
    "    mcmc.run(rng_key, X, Y, D_H, extra_fields=(\"num_steps\",))\n",
    "    elapsed = time.time() - start\n",
    "    num_steps = mcmc.get_extra_fields()[\"num_steps\"]\n",
    "    geps = sum(num_steps) / (args.num_samples * args.num_chains)\n",
    "    mcmc.print_summary()\n",
    "    print(f\"\\nNUTS elapsed time: {elapsed:.4}, grad evals per sample: {geps:.4}\")\n",
    "    return mcmc.get_samples(group_by_chain=True)\n",
    "\n",
    "\n",
    "def run_inference_lmc(model, args, rng_key, X, Y, D_H):\n",
    "    n, chains, warmup = args.num_samples, args.num_chains, args.num_warmup\n",
    "    start = time.time()\n",
    "    samples, geps = run_lmc_numpyro(\n",
    "        rng_key,\n",
    "        model,\n",
    "        (X, Y, D_H),\n",
    "        num_particles=chains,\n",
    "        chain_len=n,\n",
    "        warmup_mult=warmup,\n",
    "        tol=args.tol,\n",
    "        chain_sep=0.25,\n",
    "        warmup_tol_mult=16.0,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"LMC elapsed time: {elapsed:.4}, grad evals per sample: {geps:.4}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "# helper function for prediction\n",
    "def predict(model, rng_key, samples, X, D_H):\n",
    "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
    "    return model_trace[\"Y\"][\"value\"]\n",
    "\n",
    "\n",
    "# create artificial regression dataset\n",
    "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
    "    D_Y = 1  # create 1d outputs\n",
    "    np.random.seed(0)\n",
    "    X = jnp.linspace(-1, 1, N)\n",
    "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
    "    W = 0.5 * np.random.randn(D_X)\n",
    "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
    "    Y += sigma_obs * np.random.randn(N)\n",
    "    Y = Y[:, np.newaxis]\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y /= jnp.std(Y)\n",
    "\n",
    "    assert X.shape == (N, D_X)\n",
    "    assert Y.shape == (N, D_Y)\n",
    "\n",
    "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
    "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
    "\n",
    "    return X, Y, X_test\n",
    "\n",
    "\n",
    "def plot(predictions, X, Y, X_test, use_lmc=False):\n",
    "    # compute mean prediction and confidence interval around median\n",
    "    mean_prediction = jnp.mean(predictions, axis=0)\n",
    "    percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)\n",
    "\n",
    "    # make plots\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
    "\n",
    "    # plot training data\n",
    "    ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
    "    # plot 90% confidence level of predictions\n",
    "    ax.fill_between(\n",
    "        X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
    "    )\n",
    "    # plot mean prediction\n",
    "    ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
    "    ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n",
    "\n",
    "    filename = \"bnn_plot_lmc.pdf\" if use_lmc else \"bnn_plot.pdf\"\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def main(args, use_lmc=False):\n",
    "    N, D_X, D_H = args.num_data, 3, args.num_hidden\n",
    "    X, Y, X_test = get_data(N=N, D_X=D_X)\n",
    "    num_samples, num_chains = args.num_samples, args.num_chains\n",
    "\n",
    "    # do inference\n",
    "    rng_key, rng_key_predict = jr.split(jr.PRNGKey(0))\n",
    "\n",
    "    inference_fun = run_inference_lmc if use_lmc else run_inference\n",
    "    samples = inference_fun(model, args, rng_key, X, Y, D_H)\n",
    "\n",
    "    assert jtu.tree_all(\n",
    "        jtu.tree_map(lambda x: x.shape[:2] == (num_chains, num_samples), samples)\n",
    "    ), f\"Expected shape (num_chains, num_samples, ...) for all samples, got {jtu.tree_map(lambda x: x.shape, samples)}\"\n",
    "    flat_samples = jtu.tree_map(\n",
    "        lambda x: jnp.reshape(x, (num_samples * num_chains,) + x.shape[2:]), samples\n",
    "    )\n",
    "\n",
    "    # predict Y_test at inputs X_test\n",
    "    vmap_args = (\n",
    "        flat_samples,\n",
    "        jr.split(rng_key_predict, num_samples * num_chains),\n",
    "    )\n",
    "    predictions = jax.jit(\n",
    "        jax.vmap(\n",
    "            lambda _samples, _rng_key: predict(model, _rng_key, _samples, X_test, D_H)\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    prediction = predictions[..., 0]\n",
    "\n",
    "    plot(prediction, X, Y, X_test, use_lmc=use_lmc)\n",
    "\n",
    "    return samples, predictions\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_data=50,\n",
    "        num_hidden=5,\n",
    "        num_samples=2**9,\n",
    "        num_warmup=128,\n",
    "        num_chains=2**6,\n",
    "        tol=0.5,\n",
    "    ):\n",
    "        self.num_data = num_data\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_samples = num_samples\n",
    "        self.num_warmup = num_warmup\n",
    "        self.num_chains = num_chains\n",
    "        self.tol = tol\n",
    "\n",
    "\n",
    "def save_samples(samples, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        jnp.savez(f, **samples)\n",
    "\n",
    "\n",
    "def load_samples(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        npz = jnp.load(f)\n",
    "        samples = {k: npz[k] for k in npz.keys()}\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1fe7ea2ccd3227b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:09:15.740827Z",
     "start_time": "2024-05-11T18:02:51.735148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00%|██████████| [04:01<00:00,  2.42s/%]\n",
      "100.00%|██████████| [01:52<00:00,  1.12s/%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMC: gradient evaluations per output: 189.8\n",
      "LMC elapsed time: 383.2, grad evals per sample: 189.8\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    num_data=50,\n",
    "    num_hidden=5,\n",
    "    num_samples=2**9,\n",
    "    num_warmup=2048,\n",
    "    num_chains=2**6,\n",
    "    tol=0.1,\n",
    ")\n",
    "samples_lmc, predictions_lmc = main(args, use_lmc=True)\n",
    "save_samples(samples_lmc, \"samples_lmc.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3921858d05edf3a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:19:13.077790Z",
     "start_time": "2024-05-11T18:09:15.741484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2560/2560 [09:54<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  prec_obs     11.55      2.39     11.38      7.65     15.39  29352.87      1.00\n",
      "   w1[0,0]     -0.00      1.15      0.01     -1.88      1.88   4047.20      1.01\n",
      "   w1[0,1]     -0.01      1.15     -0.01     -1.94      1.87   3961.83      1.01\n",
      "   w1[0,2]     -0.02      1.15     -0.03     -1.89      1.89   3310.26      1.02\n",
      "   w1[0,3]      0.01      1.17      0.00     -1.87      1.95   4012.14      1.01\n",
      "   w1[0,4]      0.03      1.17      0.06     -1.88      1.92   3626.19      1.02\n",
      "   w1[1,0]     -0.00      1.14     -0.00     -1.84      1.73   3976.57      1.01\n",
      "   w1[1,1]      0.02      1.15      0.05     -1.79      1.83   3482.02      1.02\n",
      "   w1[1,2]      0.01      1.15      0.01     -1.80      1.78   3428.00      1.02\n",
      "   w1[1,3]     -0.01      1.15     -0.02     -1.79      1.82   3682.64      1.02\n",
      "   w1[1,4]     -0.00      1.14      0.01     -1.79      1.79   3704.13      1.01\n",
      "   w1[2,0]     -0.00      1.17     -0.00     -1.84      1.95   3674.53      1.01\n",
      "   w1[2,1]      0.02      1.18      0.02     -1.89      1.96   3631.79      1.02\n",
      "   w1[2,2]      0.02      1.17      0.02     -1.87      1.94   3197.79      1.02\n",
      "   w1[2,3]      0.00      1.17     -0.02     -1.82      1.96   3808.92      1.01\n",
      "   w1[2,4]     -0.00      1.17     -0.01     -1.85      1.94   3597.87      1.02\n",
      "   w2[0,0]      0.00      1.07      0.01     -1.77      1.71  16506.03      1.00\n",
      "   w2[0,1]     -0.00      1.08     -0.00     -1.78      1.73  14848.77      1.00\n",
      "   w2[0,2]      0.00      1.07      0.00     -1.71      1.79  15809.53      1.00\n",
      "   w2[0,3]      0.00      1.07     -0.00     -1.75      1.75  15840.37      1.00\n",
      "   w2[0,4]     -0.00      1.07     -0.00     -1.77      1.73  14778.34      1.00\n",
      "   w2[1,0]      0.01      1.08      0.00     -1.77      1.78  15527.91      1.00\n",
      "   w2[1,1]     -0.00      1.07     -0.00     -1.73      1.76  15936.80      1.00\n",
      "   w2[1,2]      0.01      1.08      0.02     -1.75      1.76  15416.84      1.00\n",
      "   w2[1,3]     -0.00      1.07      0.00     -1.75      1.75  16113.02      1.00\n",
      "   w2[1,4]     -0.01      1.07     -0.01     -1.73      1.76  14921.98      1.00\n",
      "   w2[2,0]      0.01      1.07      0.01     -1.70      1.80  15774.39      1.00\n",
      "   w2[2,1]      0.01      1.07      0.02     -1.77      1.72  14989.91      1.00\n",
      "   w2[2,2]      0.00      1.07      0.01     -1.74      1.76  15139.14      1.00\n",
      "   w2[2,3]     -0.01      1.08      0.01     -1.78      1.76  15321.78      1.00\n",
      "   w2[2,4]      0.00      1.07      0.01     -1.75      1.76  15328.95      1.00\n",
      "   w2[3,0]     -0.01      1.09     -0.02     -1.80      1.75  14896.63      1.00\n",
      "   w2[3,1]      0.00      1.07      0.00     -1.75      1.75  15723.14      1.00\n",
      "   w2[3,2]     -0.01      1.07     -0.01     -1.69      1.81  16067.62      1.00\n",
      "   w2[3,3]     -0.00      1.08      0.01     -1.73      1.80  14729.81      1.00\n",
      "   w2[3,4]      0.00      1.08     -0.00     -1.77      1.77  14514.19      1.00\n",
      "   w2[4,0]      0.00      1.08      0.01     -1.76      1.78  16139.04      1.00\n",
      "   w2[4,1]     -0.01      1.07     -0.01     -1.77      1.75  16614.70      1.00\n",
      "   w2[4,2]     -0.01      1.08     -0.01     -1.76      1.77  16690.98      1.00\n",
      "   w2[4,3]     -0.00      1.08      0.00     -1.81      1.72  15922.92      1.00\n",
      "   w2[4,4]      0.01      1.08      0.01     -1.72      1.80  15709.80      1.00\n",
      "   w3[0,0]     -0.01      1.42     -0.02     -2.27      2.32   5558.83      1.01\n",
      "   w3[1,0]     -0.00      1.42     -0.00     -2.28      2.31   5936.32      1.01\n",
      "   w3[2,0]      0.01      1.43      0.01     -2.25      2.35   6198.79      1.01\n",
      "   w3[3,0]      0.01      1.44      0.02     -2.34      2.31   5592.37      1.01\n",
      "   w3[4,0]     -0.01      1.43     -0.00     -2.31      2.31   5711.46      1.01\n",
      "\n",
      "Number of divergences: 84\n",
      "\n",
      "NUTS elapsed time: 596.2, grad evals per sample: 571.9\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    num_data=50, num_hidden=5, num_samples=2**9, num_warmup=2048, num_chains=2**6\n",
    ")\n",
    "samples_nuts, predictions_nuts = main(args)\n",
    "save_samples(samples_nuts, \"samples_nuts.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a40c54ae8d94f2de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:20:19.817701Z",
     "start_time": "2024-05-11T18:20:19.720477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prec_obs': 35.47085303911096, 'w1': array([[0.96547994, 0.94590134, 0.88050268, 0.7010292 , 1.08598659],\n",
      "       [1.09277597, 0.88502356, 1.02323194, 0.75362109, 0.87642086],\n",
      "       [1.1048426 , 0.75331447, 0.90245502, 0.69098364, 0.75179445]]), 'w2': array([[3.81481028, 3.35927273, 3.2468056 , 3.58500049, 3.63945121],\n",
      "       [3.33703389, 3.24539524, 3.79352066, 3.47304621, 3.61056228],\n",
      "       [3.53048479, 3.88395913, 3.55241667, 3.50269356, 3.30924641],\n",
      "       [2.79644306, 3.1397997 , 3.29180351, 2.99984435, 3.58892501],\n",
      "       [3.54133879, 3.34176407, 3.65324352, 3.29120513, 3.28289685]]), 'w3': array([[1.02754679],\n",
      "       [1.01293591],\n",
      "       [1.13488737],\n",
      "       [1.2329998 ],\n",
      "       [1.09866073]])}\n",
      "{'prec_obs': 57.329830620977035, 'w1': array([[7.90468294, 7.73795397, 6.46535417, 7.83621051, 7.08239748],\n",
      "       [7.76674254, 6.80082134, 6.69530391, 7.19266347, 7.23463372],\n",
      "       [7.17681314, 7.09333939, 6.24567394, 7.43929542, 7.02708214]]), 'w2': array([[32.23834754, 29.00150401, 30.87798748, 30.93822313, 28.86395379],\n",
      "       [30.32794627, 31.12656517, 30.11100887, 31.47074875, 29.14449506],\n",
      "       [30.80936161, 29.27717087, 29.56863241, 29.92534979, 29.93936428],\n",
      "       [29.0949861 , 30.70924923, 31.38206587, 28.76915829, 28.34803563],\n",
      "       [31.52156585, 32.45058415, 32.59957388, 31.09945711, 30.68321037]]), 'w3': array([[10.8570926 ],\n",
      "       [11.59436836],\n",
      "       [12.10702014],\n",
      "       [10.9225955 ],\n",
      "       [11.15520462]])}\n"
     ]
    }
   ],
   "source": [
    "from numpyro import diagnostics\n",
    "\n",
    "\n",
    "samples_lmc = load_samples(\"samples_lmc.npz\")\n",
    "\n",
    "ess_lmc = jtu.tree_map(\n",
    "    lambda x: diagnostics.effective_sample_size(x) / x.shape[1], samples_lmc\n",
    ")\n",
    "print(ess_lmc)\n",
    "\n",
    "samples_nuts = load_samples(\"samples_nuts.npz\")\n",
    "ess_nuts = jtu.tree_map(\n",
    "    lambda x: diagnostics.effective_sample_size(x) / x.shape[1], samples_nuts\n",
    ")\n",
    "print(ess_nuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342683f879488bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
