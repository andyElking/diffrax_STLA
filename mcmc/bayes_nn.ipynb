{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6095a29152a3dc59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T14:09:56.776965Z",
     "start_time": "2024-05-29T14:09:55.740935Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import jax.tree_util as jtu\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro import handlers\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "from mcmc import run_lmc_numpyro\n",
    "\n",
    "\n",
    "matplotlib.use(\"Agg\")  # noqa: E402\n",
    "\n",
    "\n",
    "# a two-layer bayesian neural network with computational flow\n",
    "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
    "# hidden units. (note we indicate tensor dimensions in the comments)\n",
    "def model(X, Y, D_H, D_Y=1):\n",
    "    N, D_X = X.shape\n",
    "\n",
    "    # sample first layer (we put unit normal priors on all weights)\n",
    "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
    "    assert w1.shape == (D_X, D_H), f\"Expected shape {(D_X, D_H)}, got {w1.shape}\"\n",
    "    z1 = jnp.tanh(jnp.matmul(X, w1))  # <= first layer of activations\n",
    "    assert z1.shape == (N, D_H)\n",
    "\n",
    "    # sample second layer\n",
    "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
    "    assert w2.shape == (D_H, D_H)\n",
    "    z2 = jnp.tanh(jnp.matmul(z1, w2))  # <= second layer of activations\n",
    "    assert z2.shape == (N, D_H)\n",
    "\n",
    "    # sample final layer of weights and neural network output\n",
    "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
    "    assert w3.shape == (D_H, D_Y)\n",
    "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
    "    assert z3.shape == (N, D_Y)\n",
    "\n",
    "    if Y is not None:\n",
    "        assert z3.shape == Y.shape\n",
    "\n",
    "    # we put a prior on the observation noise\n",
    "    prec_obs = numpyro.sample(\"prec_obs\", dist.Gamma(3.0, 1.0))\n",
    "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
    "\n",
    "    # observe data\n",
    "    with numpyro.plate(\"data\", N):\n",
    "        # note we use to_event(1) because each observation has shape (1,)\n",
    "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
    "\n",
    "\n",
    "# helper function for HMC inference\n",
    "def run_inference(model, args, rng_key, X, Y, D_H):\n",
    "    start = time.time()\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=args.num_warmup,\n",
    "        num_samples=args.num_samples,\n",
    "        num_chains=args.num_chains,\n",
    "        thinning=args.thinning,\n",
    "        chain_method=\"vectorized\",\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    mcmc.run(rng_key, X, Y, D_H, extra_fields=(\"num_steps\",))\n",
    "    elapsed = time.time() - start\n",
    "    num_steps = mcmc.get_extra_fields()[\"num_steps\"]\n",
    "    geps = sum(num_steps) / (args.num_samples * args.num_chains)\n",
    "    mcmc.print_summary()\n",
    "    print(f\"\\nNUTS elapsed time: {elapsed:.4}, grad evals per sample: {geps:.4}\")\n",
    "    return mcmc.get_samples(group_by_chain=True)\n",
    "\n",
    "\n",
    "def run_inference_lmc(model, args, rng_key, X, Y, D_H):\n",
    "    n, chains, warmup = args.num_samples, args.num_chains, args.num_warmup\n",
    "    start = time.time()\n",
    "    samples, geps = run_lmc_numpyro(\n",
    "        rng_key,\n",
    "        model,\n",
    "        (X, Y, D_H),\n",
    "        num_particles=chains,\n",
    "        chain_len=n,\n",
    "        warmup_mult=warmup,\n",
    "        tol=args.tol,\n",
    "        chain_sep=0.25 * args.thinning,\n",
    "        warmup_tol_mult=64.0,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"LMC elapsed time: {elapsed:.4}, grad evals per sample: {geps:.4}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "# helper function for prediction\n",
    "def predict(model, rng_key, samples, X, D_H):\n",
    "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
    "    # note that Y will be sampled in the model because we pass Y=None here\n",
    "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
    "    return model_trace[\"Y\"][\"value\"]\n",
    "\n",
    "\n",
    "# create artificial regression dataset\n",
    "def get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n",
    "    D_Y = 1  # create 1d outputs\n",
    "    np.random.seed(0)\n",
    "    X = jnp.linspace(-1, 1, N)\n",
    "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
    "    W = 0.5 * np.random.randn(D_X)\n",
    "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
    "    Y += sigma_obs * np.random.randn(N)\n",
    "    Y = Y[:, np.newaxis]\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y /= jnp.std(Y)\n",
    "\n",
    "    assert X.shape == (N, D_X)\n",
    "    assert Y.shape == (N, D_Y)\n",
    "\n",
    "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
    "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
    "\n",
    "    return X, Y, X_test\n",
    "\n",
    "\n",
    "def plot(predictions, X, Y, X_test, use_lmc=False):\n",
    "    # compute mean prediction and confidence interval around median\n",
    "    mean_prediction = jnp.mean(predictions, axis=0)\n",
    "    percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)\n",
    "\n",
    "    # make plots\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
    "\n",
    "    # plot training data\n",
    "    ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
    "    # plot 90% confidence level of predictions\n",
    "    ax.fill_between(\n",
    "        X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
    "    )\n",
    "    # plot mean prediction\n",
    "    ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
    "    ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n",
    "\n",
    "    filename = \"bnn_plot_lmc.pdf\" if use_lmc else \"bnn_plot.pdf\"\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def main(args, use_lmc=False):\n",
    "    N, D_X, D_H = args.num_data, 3, args.num_hidden\n",
    "    X, Y, X_test = get_data(N=N, D_X=D_X)\n",
    "    num_samples, num_chains = args.num_samples, args.num_chains\n",
    "\n",
    "    # do inference\n",
    "    rng_key, rng_key_predict = jr.split(jr.PRNGKey(0))\n",
    "\n",
    "    inference_fun = run_inference_lmc if use_lmc else run_inference\n",
    "    samples = inference_fun(model, args, rng_key, X, Y, D_H)\n",
    "\n",
    "    assert jtu.tree_all(\n",
    "        jtu.tree_map(lambda x: x.shape[:2] == (num_chains, num_samples), samples)\n",
    "    ), f\"Expected shape (num_chains, num_samples, ...) for all samples, got {jtu.tree_map(lambda x: x.shape, samples)}\"\n",
    "    flat_samples = jtu.tree_map(\n",
    "        lambda x: jnp.reshape(x, (num_samples * num_chains,) + x.shape[2:]), samples\n",
    "    )\n",
    "\n",
    "    # predict Y_test at inputs X_test\n",
    "    vmap_args = (\n",
    "        flat_samples,\n",
    "        jr.split(rng_key_predict, num_samples * num_chains),\n",
    "    )\n",
    "    predictions = jax.jit(\n",
    "        jax.vmap(\n",
    "            lambda _samples, _rng_key: predict(model, _rng_key, _samples, X_test, D_H)\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    prediction = predictions[..., 0]\n",
    "\n",
    "    plot(prediction, X, Y, X_test, use_lmc=use_lmc)\n",
    "\n",
    "    return samples, predictions\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_data=50,\n",
    "        num_hidden=5,\n",
    "        num_samples=2**9,\n",
    "        num_warmup=128,\n",
    "        num_chains=2**6,\n",
    "        tol=0.5,\n",
    "        thinning=1,\n",
    "    ):\n",
    "        self.num_data = num_data\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_samples = num_samples\n",
    "        self.num_warmup = num_warmup\n",
    "        self.num_chains = num_chains\n",
    "        self.tol = tol\n",
    "        self.thinning = thinning\n",
    "\n",
    "\n",
    "def save_samples(samples, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        jnp.savez(f, **samples)\n",
    "\n",
    "\n",
    "def load_samples(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        npz = jnp.load(f)\n",
    "        samples = {k: npz[k] for k in npz.keys()}\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1fe7ea2ccd3227b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:09:15.740827Z",
     "start_time": "2024-05-11T18:02:51.735148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00%|██████████| [04:01<00:00,  2.42s/%]\n",
      "100.00%|██████████| [01:52<00:00,  1.12s/%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMC: gradient evaluations per output: 189.8\n",
      "LMC elapsed time: 383.2, grad evals per sample: 189.8\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    num_data=50,\n",
    "    num_hidden=5,\n",
    "    num_samples=2**9,\n",
    "    num_warmup=2048,\n",
    "    num_chains=2**6,\n",
    "    tol=0.1,\n",
    ")\n",
    "samples_lmc, predictions_lmc = main(args, use_lmc=True)\n",
    "save_samples(samples_lmc, \"samples_lmc.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3921858d05edf3a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:19:13.077790Z",
     "start_time": "2024-05-11T18:09:15.741484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2560/2560 [09:54<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  prec_obs     11.55      2.39     11.38      7.65     15.39  29352.87      1.00\n",
      "   w1[0,0]     -0.00      1.15      0.01     -1.88      1.88   4047.20      1.01\n",
      "   w1[0,1]     -0.01      1.15     -0.01     -1.94      1.87   3961.83      1.01\n",
      "   w1[0,2]     -0.02      1.15     -0.03     -1.89      1.89   3310.26      1.02\n",
      "   w1[0,3]      0.01      1.17      0.00     -1.87      1.95   4012.14      1.01\n",
      "   w1[0,4]      0.03      1.17      0.06     -1.88      1.92   3626.19      1.02\n",
      "   w1[1,0]     -0.00      1.14     -0.00     -1.84      1.73   3976.57      1.01\n",
      "   w1[1,1]      0.02      1.15      0.05     -1.79      1.83   3482.02      1.02\n",
      "   w1[1,2]      0.01      1.15      0.01     -1.80      1.78   3428.00      1.02\n",
      "   w1[1,3]     -0.01      1.15     -0.02     -1.79      1.82   3682.64      1.02\n",
      "   w1[1,4]     -0.00      1.14      0.01     -1.79      1.79   3704.13      1.01\n",
      "   w1[2,0]     -0.00      1.17     -0.00     -1.84      1.95   3674.53      1.01\n",
      "   w1[2,1]      0.02      1.18      0.02     -1.89      1.96   3631.79      1.02\n",
      "   w1[2,2]      0.02      1.17      0.02     -1.87      1.94   3197.79      1.02\n",
      "   w1[2,3]      0.00      1.17     -0.02     -1.82      1.96   3808.92      1.01\n",
      "   w1[2,4]     -0.00      1.17     -0.01     -1.85      1.94   3597.87      1.02\n",
      "   w2[0,0]      0.00      1.07      0.01     -1.77      1.71  16506.03      1.00\n",
      "   w2[0,1]     -0.00      1.08     -0.00     -1.78      1.73  14848.77      1.00\n",
      "   w2[0,2]      0.00      1.07      0.00     -1.71      1.79  15809.53      1.00\n",
      "   w2[0,3]      0.00      1.07     -0.00     -1.75      1.75  15840.37      1.00\n",
      "   w2[0,4]     -0.00      1.07     -0.00     -1.77      1.73  14778.34      1.00\n",
      "   w2[1,0]      0.01      1.08      0.00     -1.77      1.78  15527.91      1.00\n",
      "   w2[1,1]     -0.00      1.07     -0.00     -1.73      1.76  15936.80      1.00\n",
      "   w2[1,2]      0.01      1.08      0.02     -1.75      1.76  15416.84      1.00\n",
      "   w2[1,3]     -0.00      1.07      0.00     -1.75      1.75  16113.02      1.00\n",
      "   w2[1,4]     -0.01      1.07     -0.01     -1.73      1.76  14921.98      1.00\n",
      "   w2[2,0]      0.01      1.07      0.01     -1.70      1.80  15774.39      1.00\n",
      "   w2[2,1]      0.01      1.07      0.02     -1.77      1.72  14989.91      1.00\n",
      "   w2[2,2]      0.00      1.07      0.01     -1.74      1.76  15139.14      1.00\n",
      "   w2[2,3]     -0.01      1.08      0.01     -1.78      1.76  15321.78      1.00\n",
      "   w2[2,4]      0.00      1.07      0.01     -1.75      1.76  15328.95      1.00\n",
      "   w2[3,0]     -0.01      1.09     -0.02     -1.80      1.75  14896.63      1.00\n",
      "   w2[3,1]      0.00      1.07      0.00     -1.75      1.75  15723.14      1.00\n",
      "   w2[3,2]     -0.01      1.07     -0.01     -1.69      1.81  16067.62      1.00\n",
      "   w2[3,3]     -0.00      1.08      0.01     -1.73      1.80  14729.81      1.00\n",
      "   w2[3,4]      0.00      1.08     -0.00     -1.77      1.77  14514.19      1.00\n",
      "   w2[4,0]      0.00      1.08      0.01     -1.76      1.78  16139.04      1.00\n",
      "   w2[4,1]     -0.01      1.07     -0.01     -1.77      1.75  16614.70      1.00\n",
      "   w2[4,2]     -0.01      1.08     -0.01     -1.76      1.77  16690.98      1.00\n",
      "   w2[4,3]     -0.00      1.08      0.00     -1.81      1.72  15922.92      1.00\n",
      "   w2[4,4]      0.01      1.08      0.01     -1.72      1.80  15709.80      1.00\n",
      "   w3[0,0]     -0.01      1.42     -0.02     -2.27      2.32   5558.83      1.01\n",
      "   w3[1,0]     -0.00      1.42     -0.00     -2.28      2.31   5936.32      1.01\n",
      "   w3[2,0]      0.01      1.43      0.01     -2.25      2.35   6198.79      1.01\n",
      "   w3[3,0]      0.01      1.44      0.02     -2.34      2.31   5592.37      1.01\n",
      "   w3[4,0]     -0.01      1.43     -0.00     -2.31      2.31   5711.46      1.01\n",
      "\n",
      "Number of divergences: 84\n",
      "\n",
      "NUTS elapsed time: 596.2, grad evals per sample: 571.9\n"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    num_data=50, num_hidden=5, num_samples=2**9, num_warmup=2048, num_chains=2**6\n",
    ")\n",
    "samples_nuts, predictions_nuts = main(args)\n",
    "save_samples(samples_nuts, \"samples_nuts.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afb8c35a3be1dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T18:29:33.522661Z",
     "start_time": "2024-05-29T14:10:26.118375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 67584/67584 [4:18:55<00:00,  4.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "  prec_obs     11.54      2.39     11.38      7.61     15.35 477243.52      1.00\n",
      "   w1[0,0]      0.00      1.16      0.01     -1.87      1.93 342172.61      1.00\n",
      "   w1[0,1]      0.00      1.16      0.00     -1.88      1.92 342279.81      1.00\n",
      "   w1[0,2]     -0.00      1.16     -0.00     -1.89      1.90 332054.52      1.00\n",
      "   w1[0,3]     -0.00      1.16     -0.00     -1.90      1.89 345202.48      1.00\n",
      "   w1[0,4]     -0.00      1.16     -0.00     -1.90      1.90 338795.20      1.00\n",
      "   w1[1,0]     -0.00      1.14     -0.01     -1.81      1.78 316801.26      1.00\n",
      "   w1[1,1]     -0.00      1.14     -0.01     -1.80      1.79 321233.90      1.00\n",
      "   w1[1,2]      0.00      1.15      0.00     -1.79      1.80 312331.88      1.00\n",
      "   w1[1,3]     -0.00      1.14     -0.00     -1.79      1.80 320548.45      1.00\n",
      "   w1[1,4]      0.00      1.15     -0.00     -1.80      1.79 319182.07      1.00\n",
      "   w1[2,0]     -0.01      1.17     -0.01     -1.88      1.92 306160.49      1.00\n",
      "   w1[2,1]     -0.00      1.17     -0.00     -1.91      1.88 315633.76      1.00\n",
      "   w1[2,2]      0.00      1.17      0.00     -1.89      1.91 314200.48      1.00\n",
      "   w1[2,3]     -0.00      1.17     -0.00     -1.88      1.91 315728.01      1.00\n",
      "   w1[2,4]      0.00      1.17      0.00     -1.89      1.91 310634.00      1.00\n",
      "   w2[0,0]     -0.00      1.07      0.00     -1.78      1.74 502324.42      1.00\n",
      "   w2[0,1]      0.00      1.07      0.00     -1.73      1.78 513218.92      1.00\n",
      "   w2[0,2]      0.00      1.08      0.00     -1.76      1.75 509912.20      1.00\n",
      "   w2[0,3]     -0.00      1.07      0.00     -1.76      1.76 507964.23      1.00\n",
      "   w2[0,4]     -0.00      1.07     -0.00     -1.76      1.76 510888.17      1.00\n",
      "   w2[1,0]      0.00      1.07      0.00     -1.75      1.77 505812.83      1.00\n",
      "   w2[1,1]     -0.00      1.07      0.00     -1.78      1.74 440291.02      1.00\n",
      "   w2[1,2]      0.00      1.08      0.00     -1.77      1.76 411380.47      1.00\n",
      "   w2[1,3]     -0.00      1.08     -0.00     -1.78      1.75 377694.43      1.00\n",
      "   w2[1,4]     -0.00      1.07     -0.00     -1.76      1.76 510850.98      1.00\n",
      "   w2[2,0]      0.00      1.08      0.00     -1.77      1.76 504090.27      1.00\n",
      "   w2[2,1]      0.00      1.08     -0.00     -1.76      1.76 479375.47      1.00\n",
      "   w2[2,2]      0.00      1.07      0.00     -1.77      1.75 511474.04      1.00\n",
      "   w2[2,3]     -0.00      1.07     -0.00     -1.74      1.77 487205.69      1.00\n",
      "   w2[2,4]     -0.00      1.07     -0.00     -1.75      1.75 510619.33      1.00\n",
      "   w2[3,0]      0.00      1.07      0.00     -1.76      1.76 514614.67      1.00\n",
      "   w2[3,1]     -0.00      1.08     -0.00     -1.76      1.76 494123.73      1.00\n",
      "   w2[3,2]      0.00      1.07      0.00     -1.77      1.74 514015.43      1.00\n",
      "   w2[3,3]      0.00      1.07      0.00     -1.75      1.77 375938.61      1.00\n",
      "   w2[3,4]     -0.00      1.07     -0.00     -1.75      1.77 504646.31      1.00\n",
      "   w2[4,0]     -0.00      1.07     -0.00     -1.77      1.74 513085.92      1.00\n",
      "   w2[4,1]      0.00      1.07      0.00     -1.75      1.76 493669.28      1.00\n",
      "   w2[4,2]      0.00      1.07     -0.00     -1.77      1.75 479117.59      1.00\n",
      "   w2[4,3]      0.00      1.07      0.00     -1.77      1.75 506654.13      1.00\n",
      "   w2[4,4]     -0.00      1.08     -0.00     -1.78      1.74 505139.38      1.00\n",
      "   w3[0,0]     -0.00      1.43     -0.00     -2.29      2.32 433525.55      1.00\n",
      "   w3[1,0]     -0.00      1.43     -0.00     -2.31      2.30 424403.70      1.00\n",
      "   w3[2,0]      0.00      1.43      0.00     -2.29      2.33 442516.98      1.00\n",
      "   w3[3,0]      0.00      1.43      0.00     -2.32      2.30 418446.48      1.00\n",
      "   w3[4,0]      0.00      1.43      0.00     -2.30      2.32 444773.75      1.00\n",
      "\n",
      "Number of divergences: 1828\n",
      "\n",
      "NUTS elapsed time: 1.554e+04, grad evals per sample: 72.63\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected shape (num_chains, num_samples, ...) for all samples, got {'prec_obs': (64, 8192), 'w1': (64, 8192, 3, 5), 'w2': (64, 8192, 5, 5), 'w3': (64, 8192, 5, 1)}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m args \u001B[38;5;241m=\u001B[39m Args(\n\u001B[1;32m      2\u001B[0m     num_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, num_hidden\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m16\u001B[39m, num_warmup\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m, num_chains\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m6\u001B[39m, thinning\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m\n\u001B[1;32m      3\u001B[0m )\n\u001B[0;32m----> 4\u001B[0m samples_nuts_precise, predictions_nuts_precise \u001B[38;5;241m=\u001B[39m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m save_samples(samples_nuts_precise, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples_nuts.npz\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1], line 163\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(args, use_lmc)\u001B[0m\n\u001B[1;32m    160\u001B[0m inference_fun \u001B[38;5;241m=\u001B[39m run_inference_lmc \u001B[38;5;28;01mif\u001B[39;00m use_lmc \u001B[38;5;28;01melse\u001B[39;00m run_inference\n\u001B[1;32m    161\u001B[0m samples \u001B[38;5;241m=\u001B[39m inference_fun(model, args, rng_key, X, Y, D_H)\n\u001B[0;32m--> 163\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m jtu\u001B[38;5;241m.\u001B[39mtree_all(\n\u001B[1;32m    164\u001B[0m     jtu\u001B[38;5;241m.\u001B[39mtree_map(\u001B[38;5;28;01mlambda\u001B[39;00m x: x\u001B[38;5;241m.\u001B[39mshape[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m==\u001B[39m (num_chains, num_samples), samples)\n\u001B[1;32m    165\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected shape (num_chains, num_samples, ...) for all samples, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjtu\u001B[38;5;241m.\u001B[39mtree_map(\u001B[38;5;28;01mlambda\u001B[39;00m\u001B[38;5;250m \u001B[39mx:\u001B[38;5;250m \u001B[39mx\u001B[38;5;241m.\u001B[39mshape,\u001B[38;5;250m \u001B[39msamples)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    166\u001B[0m flat_samples \u001B[38;5;241m=\u001B[39m jtu\u001B[38;5;241m.\u001B[39mtree_map(\n\u001B[1;32m    167\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: jnp\u001B[38;5;241m.\u001B[39mreshape(x, (num_samples \u001B[38;5;241m*\u001B[39m num_chains,) \u001B[38;5;241m+\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:]), samples\n\u001B[1;32m    168\u001B[0m )\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# predict Y_test at inputs X_test\u001B[39;00m\n",
      "\u001B[0;31mAssertionError\u001B[0m: Expected shape (num_chains, num_samples, ...) for all samples, got {'prec_obs': (64, 8192), 'w1': (64, 8192, 3, 5), 'w2': (64, 8192, 5, 5), 'w3': (64, 8192, 5, 1)}"
     ]
    }
   ],
   "source": [
    "args = Args(\n",
    "    num_data=50,\n",
    "    num_hidden=5,\n",
    "    num_samples=2**16,\n",
    "    num_warmup=2048,\n",
    "    num_chains=2**6,\n",
    "    thinning=8,\n",
    ")\n",
    "samples_nuts_precise, predictions_nuts_precise = main(args)\n",
    "save_samples(samples_nuts_precise, \"samples_nuts.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fb979be978828",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(predictions_nuts_precise[..., 0], X, Y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a40c54ae8d94f2de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T18:20:19.817701Z",
     "start_time": "2024-05-11T18:20:19.720477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prec_obs': 35.47085303911096, 'w1': array([[0.96547994, 0.94590134, 0.88050268, 0.7010292 , 1.08598659],\n",
      "       [1.09277597, 0.88502356, 1.02323194, 0.75362109, 0.87642086],\n",
      "       [1.1048426 , 0.75331447, 0.90245502, 0.69098364, 0.75179445]]), 'w2': array([[3.81481028, 3.35927273, 3.2468056 , 3.58500049, 3.63945121],\n",
      "       [3.33703389, 3.24539524, 3.79352066, 3.47304621, 3.61056228],\n",
      "       [3.53048479, 3.88395913, 3.55241667, 3.50269356, 3.30924641],\n",
      "       [2.79644306, 3.1397997 , 3.29180351, 2.99984435, 3.58892501],\n",
      "       [3.54133879, 3.34176407, 3.65324352, 3.29120513, 3.28289685]]), 'w3': array([[1.02754679],\n",
      "       [1.01293591],\n",
      "       [1.13488737],\n",
      "       [1.2329998 ],\n",
      "       [1.09866073]])}\n",
      "{'prec_obs': 57.329830620977035, 'w1': array([[7.90468294, 7.73795397, 6.46535417, 7.83621051, 7.08239748],\n",
      "       [7.76674254, 6.80082134, 6.69530391, 7.19266347, 7.23463372],\n",
      "       [7.17681314, 7.09333939, 6.24567394, 7.43929542, 7.02708214]]), 'w2': array([[32.23834754, 29.00150401, 30.87798748, 30.93822313, 28.86395379],\n",
      "       [30.32794627, 31.12656517, 30.11100887, 31.47074875, 29.14449506],\n",
      "       [30.80936161, 29.27717087, 29.56863241, 29.92534979, 29.93936428],\n",
      "       [29.0949861 , 30.70924923, 31.38206587, 28.76915829, 28.34803563],\n",
      "       [31.52156585, 32.45058415, 32.59957388, 31.09945711, 30.68321037]]), 'w3': array([[10.8570926 ],\n",
      "       [11.59436836],\n",
      "       [12.10702014],\n",
      "       [10.9225955 ],\n",
      "       [11.15520462]])}\n"
     ]
    }
   ],
   "source": [
    "from numpyro import diagnostics\n",
    "\n",
    "\n",
    "samples_lmc = load_samples(\"samples_lmc.npz\")\n",
    "\n",
    "ess_lmc = jtu.tree_map(\n",
    "    lambda x: diagnostics.effective_sample_size(x) / x.shape[1], samples_lmc\n",
    ")\n",
    "print(ess_lmc)\n",
    "\n",
    "samples_nuts = load_samples(\"samples_nuts.npz\")\n",
    "ess_nuts = jtu.tree_map(\n",
    "    lambda x: diagnostics.effective_sample_size(x) / x.shape[1], samples_nuts\n",
    ")\n",
    "print(ess_nuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342683f879488bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
